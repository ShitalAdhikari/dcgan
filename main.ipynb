{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of project:\n",
    "The project performs the implementation of DCGAN architecture where generator tries to create a new MNIST data from the given latent  \n",
    "\n",
    "[Paper Link](https://arxiv.org/pdf/1511.06434.pdf)\n",
    "\n",
    "\n",
    "Here, the generator takes the latent vector of given input size.\n",
    "\n",
    "This is transformed to the 784 size vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General config\n",
    "batch_size = 64\n",
    "\n",
    "# Generator config\n",
    "sample_size = 100    \n",
    "g_alpha     = 0.01   \n",
    "g_lr        = 1.0e-3 \n",
    "\n",
    "# Discriminator config\n",
    "d_alpha = 0.01       \n",
    "d_lr    = 1.0e-4     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generator network with transposed convolutions\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, sample_size: int, alpha: float):\n",
    "        super().__init__()\n",
    "        # sample_size => 784 \n",
    "\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.l1 = nn.Linear(sample_size, 784)\n",
    "        self.bn1 = nn.BatchNorm1d(784)\n",
    "\n",
    "        #\n",
    "        self.conv1 = nn.ConvTranspose2d(16, 32, \n",
    "                               kernel_size=5, stride=2, padding=2,\n",
    "                               output_padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(32, 1,\n",
    "                               kernel_size=5, stride=2, padding=2,\n",
    "                               output_padding=1, bias=False)\n",
    "\n",
    "        # Random value vector size\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "\n",
    "    def forward(self, batch_size: int):\n",
    "        # Random value generation\n",
    "        z = torch.randn(batch_size, self.sample_size)\n",
    "        # print(z.shape)\n",
    "\n",
    "        # the linear layer extracts the 784 features\n",
    "        x = nn.LeakyReLU(self.alpha)(self.bn1(self.l1(z)))\n",
    "        # print(x.shape)\n",
    "\n",
    "        # the 784 vector size is reshaped to 16*7*7, where 7*7 acts as the image size and 16 as the number of features\n",
    "        x = torch.reshape(x, (-1, 16,7,7)) \n",
    "\n",
    "        # 16*7*7 is converted to the 32*14*14. 14*14 scale up image comes from the increased kernel size of 5 during TransposeConvolution\n",
    "        x = nn.LeakyReLU(self.alpha)(self.bn2(self.conv1(x)))   \n",
    "\n",
    "        # \n",
    "        # print(x.shape)\n",
    "\n",
    "        # again the transpose convolution is used to convert change image size to 28*28 with 1 features.\n",
    "        # application of sigmoid fixes the layer output to the range of the 0 to 1\n",
    "        x = nn.Sigmoid()(self.conv2(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Generator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5550, 0.3139, 0.5574,  ..., 0.6970, 0.4372, 0.5643],\n",
      "          [0.7290, 0.4817, 0.4791,  ..., 0.5538, 0.5126, 0.4993],\n",
      "          [0.6033, 0.3975, 0.5155,  ..., 0.4286, 0.5637, 0.4460],\n",
      "          ...,\n",
      "          [0.6212, 0.6079, 0.2619,  ..., 0.5273, 0.5170, 0.4668],\n",
      "          [0.5250, 0.4222, 0.3725,  ..., 0.5432, 0.4824, 0.4563],\n",
      "          [0.5132, 0.5646, 0.5117,  ..., 0.5214, 0.5228, 0.5218]]],\n",
      "\n",
      "\n",
      "        [[[0.6906, 0.4980, 0.4349,  ..., 0.4163, 0.6048, 0.5199],\n",
      "          [0.5561, 0.5428, 0.4523,  ..., 0.6602, 0.5158, 0.5214],\n",
      "          [0.5430, 0.4391, 0.5007,  ..., 0.4026, 0.3523, 0.5505],\n",
      "          ...,\n",
      "          [0.5953, 0.5025, 0.2096,  ..., 0.5335, 0.5460, 0.4824],\n",
      "          [0.3004, 0.3751, 0.3450,  ..., 0.5677, 0.4928, 0.4548],\n",
      "          [0.5130, 0.5716, 0.2769,  ..., 0.4182, 0.5240, 0.5058]]],\n",
      "\n",
      "\n",
      "        [[[0.4692, 0.4366, 0.2013,  ..., 0.1029, 0.5057, 0.4172],\n",
      "          [0.5711, 0.5580, 0.6006,  ..., 0.4828, 0.4866, 0.6276],\n",
      "          [0.7408, 0.9064, 0.3710,  ..., 0.1766, 0.6340, 0.6465],\n",
      "          ...,\n",
      "          [0.1887, 0.5104, 0.8067,  ..., 0.1360, 0.7401, 0.6182],\n",
      "          [0.2267, 0.6296, 0.6260,  ..., 0.4606, 0.4998, 0.4645],\n",
      "          [0.4345, 0.3475, 0.7270,  ..., 0.3885, 0.5536, 0.5767]]],\n",
      "\n",
      "\n",
      "        [[[0.4443, 0.5517, 0.6052,  ..., 0.7858, 0.3162, 0.4587],\n",
      "          [0.3863, 0.5834, 0.1641,  ..., 0.1926, 0.3202, 0.3694],\n",
      "          [0.5493, 0.4719, 0.6558,  ..., 0.7800, 0.6808, 0.4099],\n",
      "          ...,\n",
      "          [0.5945, 0.5025, 0.3700,  ..., 0.5683, 0.2163, 0.6619],\n",
      "          [0.5737, 0.3817, 0.5526,  ..., 0.4296, 0.4412, 0.5531],\n",
      "          [0.5369, 0.7219, 0.5173,  ..., 0.7084, 0.5895, 0.5212]]],\n",
      "\n",
      "\n",
      "        [[[0.6050, 0.1907, 0.2093,  ..., 0.3588, 0.6357, 0.6170],\n",
      "          [0.2468, 0.2936, 0.7709,  ..., 0.7520, 0.8245, 0.5364],\n",
      "          [0.5875, 0.3638, 0.5523,  ..., 0.5357, 0.5504, 0.3447],\n",
      "          ...,\n",
      "          [0.1624, 0.1924, 0.5547,  ..., 0.5082, 0.6044, 0.4696],\n",
      "          [0.5837, 0.6251, 0.5617,  ..., 0.3647, 0.7082, 0.4903],\n",
      "          [0.4222, 0.3947, 0.6393,  ..., 0.5330, 0.4908, 0.3697]]]],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = Generator(4, 0.5)\n",
    "print(a(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator network with convolutions\n",
    "# the goal of the descriminator is used to classify whether the image is real or fake and is tested\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, alpha: float):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32,\n",
    "                    kernel_size=5, stride=2, padding=2, bias=False)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 16,\n",
    "                    kernel_size=5, stride=2, padding=2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.l1 = nn.Linear(784, 784)\n",
    "        self.bn2 = nn.BatchNorm1d(784)\n",
    "        \n",
    "        self.l2 = nn.Linear(784, 1)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, images: torch.Tensor, targets: torch.Tensor):\n",
    "\n",
    "        # the input image is convolved and the image size is also reduced during the convolution without the use of maxpooling\n",
    "        # the paper mentions the removal of the max pooling layer with the convolution layer\n",
    "        # this further makes the model learn the parameters even during the pooling operation\n",
    "\n",
    "        # given the input image is 28 * 28 * 1 -> output size of 32 * 14 * 14 is extracted\n",
    "        x = nn.LeakyReLU(self.alpha)(self.conv1(images))  \n",
    "        \n",
    "\n",
    "        # further the size is reduced to 16 * 7 * 7\n",
    "        x = nn.LeakyReLU(self.alpha)(self.bn1(self.conv2(x)))       \n",
    "        \n",
    "\n",
    "        # the input is flattened to 784 size then fed to linear layer\n",
    "        x = nn.Flatten()(x)\n",
    "        \n",
    "        x = self.bn2(self.l1(x))\n",
    "        \n",
    "        x = nn.LeakyReLU(self.alpha)(x)\n",
    "        \n",
    "        # finally the single dimension output is extracted.\n",
    "        prediction = self.l2(x) \n",
    "\n",
    "        # bcelogits is returned as the loss\n",
    "        loss = F.binary_cross_entropy_with_logits(prediction, targets)\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Descriminator Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7647, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "des = Discriminator(alpha=0.5)\n",
    "\n",
    "images = torch.randn((2,1,28,28))\n",
    "targets = torch.zeros((2,1))\n",
    "print(des(images, targets))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading mnist datgaset from torch library\n",
    "transform = transforms.ToTensor()\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save the image\n",
    "def save_image_grid(epoch: int, images: torch.Tensor, ncol: int, file_dir = \"data\"):\n",
    "    os.makedirs(file_dir, exist_ok=True)\n",
    "    image_grid = make_grid(images, ncol)     # Images in a grid\n",
    "    image_grid = image_grid.permute(1, 2, 0) # Move channel last\n",
    "    image_grid = image_grid.cpu().numpy()    # To Numpy\n",
    "\n",
    "    plt.imshow(image_grid)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig(f'{file_dir}/generated_{epoch:03d}.jpg')\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- creating the real targets are ones while the fake targets are zeros.\n",
    "- both architecture is optimized using the Adam optimizer.\n",
    "- at first the descriminator is trained and then the descriminator is evaluated with generator \n",
    "- at the end each epoch, random image is generated using generator and saved to data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:18<00:00, 50.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9215868586473557 1.1129864389384951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:18<00:00, 51.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9431092013925982 1.0928135266171703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:18<00:00, 49.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.8891290809835924 1.1588647991514256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:19<00:00, 48.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.8256860332148053 1.2514866931461346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 937/937 [00:19<00:00, 47.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.7860277973854937 1.324696316217791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 813/937 [00:19<00:02, 42.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m     62\u001b[0m     d_loss \u001b[39m=\u001b[39m train_descriminator()\n\u001b[0;32m---> 63\u001b[0m     g_loss \u001b[39m=\u001b[39m train_generator()\n\u001b[1;32m     65\u001b[0m     \u001b[39m# Keep losses for logging\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     d_losses\u001b[39m.\u001b[39mappend(d_loss)\n",
      "Cell \u001b[0;32mIn[9], line 50\u001b[0m, in \u001b[0;36mtrain_generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m# Optimizer updates the generator parameters\u001b[39;00m\n\u001b[1;32m     49\u001b[0m g_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 50\u001b[0m g_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     51\u001b[0m g_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m \u001b[39mreturn\u001b[39;00m g_loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m/home/shital/learning/uccs/dcgan/dcgan_venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/home/shital/learning/uccs/dcgan/dcgan_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Real and fake labels\n",
    "real_targets = torch.ones(batch_size, 1)\n",
    "fake_targets = torch.zeros(batch_size, 1)\n",
    "\n",
    "# Generator and discriminator networks\n",
    "generator = Generator(sample_size, g_alpha)\n",
    "discriminator = Discriminator(d_alpha)\n",
    "\n",
    "# Optimizers\n",
    "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=d_lr)\n",
    "g_optimizer = torch.optim.Adam(generator.parameters(), lr=g_lr)\n",
    "\n",
    "\n",
    "def train_descriminator():\n",
    "\n",
    "    # Loss with MNIST image inputs and real_targets as labels\n",
    "    discriminator.train()\n",
    "    d_loss = discriminator(images, real_targets)\n",
    "    \n",
    "    # Generate images in eval mode\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        generated_images = generator(batch_size)\n",
    "\n",
    "    # Loss with generated image inputs and fake_targets as labels\n",
    "    d_loss += discriminator(generated_images, fake_targets)\n",
    "\n",
    "    # Optimizer updates the discriminator parameters\n",
    "    d_optimizer.zero_grad()\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "\n",
    "    return d_loss.item()\n",
    "\n",
    "def train_generator():\n",
    "\n",
    "    # Generate images in train mode\n",
    "    generator.train()\n",
    "    generated_images = generator(batch_size)\n",
    "\n",
    "    # batchnorm is unstable in eval due to generated images\n",
    "    # change drastically every epoch. We'll not use the eval here.\n",
    "    # discriminator.eval() \n",
    "\n",
    "    # Loss with generated image inputs and real_targets as labels\n",
    "    g_loss = discriminator(generated_images, real_targets)\n",
    "\n",
    "    # Optimizer updates the generator parameters\n",
    "    g_optimizer.zero_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "\n",
    "    return g_loss.item()\n",
    "\n",
    "for epoch in range(100):\n",
    "\n",
    "    d_losses = []\n",
    "    g_losses = []\n",
    "\n",
    "    for images, labels in tqdm(dataloader):\n",
    "\n",
    "        d_loss = train_descriminator()\n",
    "        g_loss = train_generator()\n",
    "\n",
    "        # Keep losses for logging\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "    # Print average losses\n",
    "    print(epoch, np.mean(d_losses), np.mean(g_losses))\n",
    "\n",
    "    # Save images\n",
    "    save_image_grid(epoch, generator(batch_size), ncol=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcgan_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
